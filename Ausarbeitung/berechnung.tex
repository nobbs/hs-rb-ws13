%!TEX root = main.tex

\section{Berechnung des Fehlerschätzers} % (fold)
\label{sec:berechnung_des_fehlersch_tzers}

Wir wollen uns nun mit der tatsächlichen Berechnung der Fehlerschätzer befassen.
Dazu werden wir uns zuerst $\alpha_\text{LB}(\mu)$ widmen und danach eine Offline-Online-Berechnung für $\norm{\hat e(\mu)}_X$ herleiten.

\subsection{Untere Schranke für die Koerzivitätskonstante} % (fold)
\label{sub:untere_schranke_f_r_die_koerzivit_tskonstante}
Wir bleiben weiterhin bei der Einschränkung, dass $a$ \emph{compliant}, also symmetrisch und parametrisch koerziv (vgl Anfang von Abschnitt \ref{sub:herleitung}), ist.

Wir definieren eine Funktion $\Theta_a^{\min,\bar \mu} \colon \mathcal D \to \mathbb{R}_+$ durch
\begin{equation}
    \Theta_a^{\min, \bar \mu}(\mu) = \min_{k = 1 \ldots Q_a} \frac{\Theta_a^k(\mu)}{\Theta_a^k(\bar \mu)}.
\end{equation}
Diese ist, da die $\Theta_a^k$ oftmals einfache algebraische Ausdrücke sind, im Allgemeinen schnell und effizient auswertbar.

\begin{Satz}
    Es gilt
    \begin{equation}
        0 < \Theta_a^{\min, \bar \mu}(\mu) \leq \alpha(\mu), \qquad \forall \mu \in \mathcal D.
    \end{equation}

    \begin{Beweis}
        Betrachte für $w \in X$ und $\mu \in \mathcal D$  beliebig
        \begin{align}
            a(w, w; \mu)
            &= \sum_{k=1}^{Q_a} \Theta_a^k(\mu) a^k(w, w)
            = \sum_{k=1}^{Q_a} \frac{\Theta_a^k(\mu)}{\Theta_a^k(\bar \mu)} \Theta_a^k(\bar \mu) a^k(w, w)
            \\
            &\geq \min_{k = 1 \ldots Q_a} \frac{\Theta_a^k(\mu)}{\Theta_a^k(\bar \mu)} a(w, w; \bar \mu)
            = \Theta_a^{\min, \bar \mu}(\mu) \norm{w}_X^2.
        \end{align}
        Daraus ergibt sich
        \begin{equation}
            \alpha(\mu) = \inf_{w \in X} \frac{a(w, w; \mu)}{\norm{w}^2_X} \geq \Theta_a^{\min, \bar \mu}(\mu) > 0
        \end{equation}
        für alle $\mu \in \mathcal D$.
    \end{Beweis}
\end{Satz}

Setzen wir nun
\begin{equation}
    \alpha_\text{LB}(\mu) := \Theta_a^{\min, \bar \mu}(\mu),
\end{equation}
so erhalten wir die für unseren Fehlerschätzer notwendige untere Schranke für die Koerzivitätskonstante $\alpha(\mu)$.

Dieses einfache Verfahren funktioniert nur bei parametrisch koerziven Bilinearformen. Ist $a$ nun koerziv, aber nicht parametrisch koerziv, dann können wir mittels der sogenannten \emph{Succesive Constraint Method}, SCM, durch etwas mehr Aufwand ebenfalls eine untere Schranke bestimmen. Dazu betrachtet man
\begin{equation}
    \mathcal Y = \left\{ (y_1, ..., y_{Q_a}) \in \mathbb{R}^{Q_a} \mid \exists w_y \in X \text{ s.d. } y_k = \frac{a^k(w_y, w_y)}{\norm{w_y}^2_X}, 1 \leq k \leq Q_a \right\}
\end{equation}
und eine Abbildung $\mathcal I \colon \mathcal D \times \mathbb{R}^{Q_a} \to \mathbb{R}$, $\mathcal I(\mu, y) = \sum_{k=1}^{Q_a} \Theta^k_a(\mu) y_k$, denn es gilt $\alpha(\mu) = \min_{y \in \mathcal Y} \mathcal I(\mu, y)$.
Mit einer Kombination aus Greedy-Verfahren und linearer Programmierung lassen sich dann Mengen $\mathcal Y_\text{UB} \subset \mathcal Y \subset \mathcal Y_\text{LB}$ und durch $\alpha_\text{LB}(\mu) = \min_{y \in \mathcal Y_\text{LB}} \mathcal I(\mu, y)$ die gesuchte Schranke finden.

Diese Succesive Constraint Method lässt sich nach einigen Abänderungen auch auf Variationsprobleme anwenden, welche die inf-sup-Bedingung erfüllen.

% subsection untere_schranke_f_r_die_koerzivit_tskonstante (end)

\subsection{Berechnung und Online-/Offline-Zerlegung der Residuumsnorm} % (fold)
\label{sub:online_offline_zerlegung_von_norm}

Wir wollen nun die Norm des Residuum bestimmen. Dazu fordern wir von der Bilinearform $a$, dass diese symmetrisch, parametrisch affin und koerziv ist.

Wir schreiben die Basis von $X$ als $\left\{ \phi_i \mid i = 1 \ldots \mathcal N \right\}$ und die Basis von $X_N$ als $\left\{ \zeta_i \mid i = 1 \ldots N \right\}$.
Sei die Massematrix $\mathbb{X}$ gegeben durch
\begin{equation}
    \mathbb{X} = \left[ \skprod{\phi_i}{\phi_j}_X \right]_{i,j=1}^{\mathcal N}.
\end{equation}

Die parametrische Affinität liefert uns für $v \in X$ beliebig
\begin{equation}
    \label{eq:skprod_e_v}
    \skprod{\hat e(\mu)}{v} = r(v; \mu) = \sum_{k=1}^{Q_f} \Theta_f^k(\mu) f^k(v) - \sum_{k=1}^{Q_a} \sum_{n=1}^{N} \Theta_a^k(\mu) u_N^{(n)}(\mu)a^k(\zeta_n, v).
\end{equation}
Betrachten wir $a^k(\zeta_n, \cdot)$ als Abbildung, dann ist diese ein stetiges lineares Funktional. Verwendung des Rieszschen Darstellungssatzes liefert uns $\hat f^k \in X$, $1 \leq k \leq Q_f$, und $\hat a^k_n \in X$, $1 \leq k \leq Q_a$, $1 \leq n \leq N$, sodass
\begin{equation}
    f^k(v) = \skprod{\hat f^k}{v}_X, \qquad a^k(\zeta_n, v) = \skprod{\hat a^k_n}{v}_X, \qquad \forall v \in X,
\end{equation}
woraus wir folgende Gleichungssysteme erhalten
\begin{equation}
    \label{eq:Xf_eq_f_and_Xa_eq_a}
    \mathbb{X} \hat f^k = [f^k(\phi_i)]_{i=1}^{\mathcal N} =: \underline f^k, \quad \mathbb{X} \hat a^k_n = [a^k(\zeta_n, \phi_i)]_{i=1}^{\mathcal N} =: \underline a^k_n.
\end{equation}
Mit Hilfe der Linearität des $X$-Skalarprodukts können wir $\hat e(\mu)$ schreiben als
\begin{equation}
    \hat e(\mu) = \sum_{k=1}^{Q_f} \Theta_f^k(\mu) \hat f^k - \sum_{k=1}^{Q_a} \sum_{n=1}^{N} \Theta_a^k(\mu) u_N^{(n)}(\mu) \hat a^k_n,
\end{equation}
eingesetzt in \eqref{eq:skprod_e_v} ergibt dies
\begin{align}
    \label{eq:residumnorm}
    \norm{\hat e(\mu)}^2_X
    &= \skprod{\hat e(\mu)}{\hat e(\mu)}_X \\
    &=
    \begin{aligned}[t]
    &\sum_{k, j = 1}^{Q_f} \Theta_f^k(\mu) \Theta_f^j(\mu) \skprod{\hat f^k}{\hat f^j}_X - 2 \sum_{k = 1}^{Q_f} \sum_{j = 1}^{Q_a} \sum_{n = 1}^N \Theta_f^k(\mu) \Theta_a^j(\mu) u_N^{(n)}(\mu) \skprod{\hat a^j_n}{\hat f^k}_X \\
    + &\sum_{k, j = 1}^{Q_a} \sum_{n, m = 1}^N \Theta_a^k(\mu) \Theta_a^j(\mu) u_N^{(n)}(\mu)u_N^{(m)}(\mu) \skprod{\hat a_n^k}{\hat a_m^j}_X.
    \end{aligned}
\end{align}
Da $a$ symmetrisch und koerziv ist, insbesondere also positiv definit, ist $\mathbb{X}$ invertierbar und wir erhalten durch Verwendung von \eqref{eq:Xf_eq_f_and_Xa_eq_a}
\begin{equation}
    \skprod{\hat f^k}{\hat f^j}_X = (\hat f^k)\Transp \, \mathbb{X} \, \hat f^j = (\underline f^k)\Transp \, \mathbb{X}^{-1} \, \underline f^j
\end{equation}
und analog
\begin{equation}
    \skprod{\hat a^k_n}{\hat f^j}_X = (\underline a^k_n)\Transp \, \mathbb{X}^{-1} \, \underline f^j,
    \quad \skprod{\hat a^k_n}{\hat a^j_m}_X = (\underline a^k_n)\Transp \, \mathbb{X}^{-1} \, \underline a^j_m.
\end{equation}
Damit lässt dich die Residuumsnorm nun berechnen. Zugleich erhalten wir dadurch auch eine Online-/Offline-Zerlegung. In der Offline-Phase berechnen wir $(\underline f^k)\Transp \, \mathbb{X}^{-1} \, \underline f^j$, $(\underline a^k_n)\Transp \, \mathbb{X}^{-1} \, \underline f^j$ und $(\underline a^k_n)\Transp \, \mathbb{X}^{-1} \, \underline a^j_m$. In der Online-Phase kann dann \eqref{eq:residumnorm} durch einige simple Additionen und Multiplikationen ausgewertet werden.

% ToDo: Matrix-Schreibweise auch noch machen?

% subsection online_offline_zerlegung_von_norm (end)

% section berechnung_des_fehlersch_tzers (end)
